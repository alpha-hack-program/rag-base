app: rag-lsd
partOf: rag-base

namespace: rag-base
# createNamespace: true

argocdNamespace: openshift-gitops

milvusDbPath: ~/.llama/distributions/rh/milvus.db

fmsOchestratorUrl: http://localhost

lsdImage: quay.io/opendatahub/llama-stack:odh
lsdPort: 8321

lsdPlaygroundImage: quay.io/rh-aiservices-bu/llama-stack-playground:0.2.11

lsdResources:
  limits:
    cpu: '2'
    memory: 12Gi
  requests:
    cpu: 250m
    memory: 500Mi

mcpServers:
  - id: mcp::bon-calculadora
    provider_id: model-context-protocol
    endpoint:
      uri: "http://bon-calculadora:8000/sse"

# mcpServers:
#   - id: mcp::bon-calculadora
#     provider_id: model-context-protocol
#     endpoint:
#       uri: "http://bon-calculadora:8000/sse"

# models:
#   - name: granite-3-3-8b
#     url: http://granite-3-3-8b-predictor:8080/v1
#     model: granite-3-3-8b-instruct
#     api_key: ""
#     tls_verify: false
#     max_tokens: 12000
#   - name: llama-3-1-8b-w4a16
#     url: http://llama-3-1-8b-w4a16-predictor:8080/v1
#     model: meta-llama/Llama-3.1-8B-Instruct
#     api_key: ""
#     tls_verify: false
#     max_tokens: 12000

prompts:
  - name: context
    description: "Prompt for answering general questions with context"
    template: |
      Given the following context:
      <context>
      {context}
      </context>

      Answer the question: {query}
      Don't use any information outside the context provided. Don't make up any information. If you don't know the answer, just say 'I don't know'.

  - name: default
    description: "Prompt for answering questions without context"
    template: |
      Answer the question: {query}
      Don't use any context. Don't make up any information. If you don't know the answer, just say 'I don't know'.
  
