app: rag-base

dataScienceProjectDisplayName: rag-base
dataScienceProjectNamespace: rag-base

instanceName: rag-base

argocdNamespace: openshift-gitops

vcs:
  uri: https://github.com/alpha-hack-program/rag-base.git
  ref: main
  name: alpha-hack-program/rag-base

embeddingsDefaultModel: multilingual-e5-large-gpu

embeddings:
  - name: multilingual-e5-large-gpu
    displayName: multilingual-e5-large GPU
    model: intfloat/multilingual-e5-large
    image: quay.io/atarazana/modelcar-catalog:multilingual-e5-large
    maxModelLen: '512'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '2'
          memory: 8Gi
        requests:
          cpu: '1'
          memory: 4Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G

models:
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    id: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '15000'
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
    args:
      - '--enable-auto-tool-choice'
      - '--tool-call-parser'
      - 'granite'
      - '--chat-template'
      - '/app/data/template/tool_chat_template_granite.jinja'
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    id: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '15000' # vllm max model len
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /app/data/template/tool_chat_template_llama3.1_json.jinja

milvusApplication:
  name: milvus
  path: gitops/milvus
  targetRevision: main

pipelinesApplication:
  name: pipelines
  path: gitops/pipelines
  targetRevision: main
  vcs: # This is the VCS configuration for the KFP pipelines code
    uri: https://github.com/alpha-hack-program/rag-utils.git
    ref: main
    name: alpha-hack-program/rag-utils
    dir: pipelines

routerApplication:
  name: rag-router
  path: gitops/rag-router
  targetRevision: main

documentsConnection:
  name: documents
  displayName: documents
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: documents
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

pipelinesConnection:
  name: pipelines
  displayName: pipelines
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: pipelines
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

webuiApplication:
  name: webui
  openaiApiKey: "1234"
  secretKey: "1234"

lsdApplication:
  name: lsd
  path: gitops/rag-lsd
  targetRevision: main
  resources:
    limits:
      cpu: '2'
      memory: 12Gi
    requests:
      cpu: 250m
      memory: 500Mi

mcpServersApplication:
  name: mcp-servers
  path: gitops/mcp-servers
  targetRevision: main

mcpServers:
  - id: mcp::bon-calculadora
    provider_id: model-context-protocol
    vcs:
      uri: https://github.com/alpha-hack-program/bon-calculadora-mcp-js.git
      ref: main
      path: .
    image: quay.io/atarazana/bon-calculadora-mcp-js:1.0.0
    mcp_transport: "sse"
    protocol: "http"
    host: bon-calculadora
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: '2'
        memory: 4Gi
      requests:
        cpu: 250m
        memory: 500Mi

minio:
  name: minio
  namespace: ic-shared-minio

documentsMilvusConnection:
  name: documents
  collectionName: documents

milvus:
  name: milvus
  username: root
  password: Milvus
  port: '19530'
  host: vectordb-milvus
  database: default
  collectionName: document_chunks

setup:
  image: quay.io/atarazana/hf-cli:latest

mountCaCerts: "false"