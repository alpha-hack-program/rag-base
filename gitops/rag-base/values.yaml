app: rag-base

argocdNamespace: openshift-gitops

vcs:
  uri: https://github.com/alpha-hack-program/rag-base.git
  ref: main
  name: alpha-hack-program/rag-base

embeddings:
  - name: multilingual-e5-large-gpu
    displayName: multilingual-e5-large GPU
    model: intfloat/multilingual-e5-large
    image: quay.io/atarazana/modelcar-catalog:multilingual-e5-large
    maxModelLen: '512'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '2'
          memory: 8Gi
        requests:
          cpu: '1'
          memory: 4Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G

models:
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    model: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '6000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
  - name: mistral-7b-instruct-v0-3
    displayName: Mistral 7B Instruct v0.3
    model: mistralai/Mistral-7B-Instruct-v0.3
    image: quay.io/redhat-ai-services/modelcar-catalog:mistral-7b-instruct-v0.3
    maxModelLen: '4000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '6000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G

# modelApplication:
#   name: doc-bot-model
#   repoURL: https://github.com/alpha-hack-program/model-serving-utils.git
#   path: gitops/model
#   targetRevision: main
#   model:
#     root: mistralai
#     id: Mistral-7B-Instruct-v0.2
#     name: mistral-7b
#     displayName: Mistral 7b
#     maxReplicas: 1
#     enableAuth: false
#     rawDeployment: false
#     runtime:
#       templateName: vllm-serving-template
#       templateDisplayName: vLLM Serving Template
#       image: quay.io/modh/vllm:rhoai-2.20-cuda
#       resources:
#         limits:
#           cpu: '8'
#           memory: 24Gi
#         requests:
#           cpu: '6'
#           memory: 24Gi
#     accelerator:
#       max: '1'
#       min: '1'
#       productName: NVIDIA-A10G
#     connection:
#       name: llm
#       createSecret: true
#       displayName: LLM
#       type: s3
#       scheme: http
#       awsAccessKeyId: minio
#       awsSecretAccessKey: minio123
#       awsDefaultRegion: none
#       awsS3Bucket: models
#       awsS3Endpoint: minio.ic-shared-minio.svc:9000
#     volumes:
#       shm:
#         sizeLimit: 2Gi

# embeddingsApplication:
#   name: doc-bot-embeddings
#   repoURL: https://github.com/alpha-hack-program/model-serving-utils.git
#   path: gitops/model
#   targetRevision: main
#   model:
#     root: intfloat
#     id: multilingual-e5-large
#     name: multilingual-e5-large-gpu
#     displayName: multilingual-e5-large GPU
#     maxReplicas: 1
#     format: vLLM
#     maxModelLen: '512'
#     apiProtocol: REST
#     enableAuth: false
#     rawDeployment: true
#     runtime:
#       templateName: vllm-serving-template
#       templateDisplayName: vLLM Serving Template
#       image: quay.io/modh/vllm:rhoai-2.20-cuda
#       resources:
#         limits:
#           cpu: '2'
#           memory: 8Gi
#         requests:
#           cpu: '1'
#           memory: 4Gi
#     accelerator:
#       max: '1'
#       min: '1'
#       productName: NVIDIA-A10G
#     connection:
#       name: embeddings-svc
#       createSecret: true
#       displayName: embeddings-svc
#       type: s3
#       scheme: http
#       awsAccessKeyId: minio
#       awsSecretAccessKey: minio123
#       awsDefaultRegion: none
#       awsS3Bucket: models
#       awsS3Endpoint: minio.ic-shared-minio.svc:9000
#     volumes:
#       shm:
#         sizeLimit: 2Gi

milvusApplication:
  name: milvus
  path: gitops/milvus
  targetRevision: main

pipelinesApplication:
  name: pipelines
  path: gitops/pipelines
  targetRevision: main

routerApplication:
  name: router
  path: gitops/rag-router
  targetRevision: main

dataScienceProjectDisplayName: rag-base
dataScienceProjectNamespace: rag-base

instanceName: rag-base

documentsConnection:
  name: documents
  displayName: documents
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: documents
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

pipelinesConnection:
  name: pipelines
  displayName: pipelines
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: pipelines
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

minio:
  name: minio
  namespace: ic-shared-minio

documentsMilvusConnection:
  name: documents
  collectionName: documents

milvus:
  name: milvus
  namespace: milvus
  username: root
  password: Milvus
  port: '19530'
  host: vectordb-milvus
  database: default

setup:
  image: quay.io/atarazana/hf-cli:latest

mountCaCerts: "false"